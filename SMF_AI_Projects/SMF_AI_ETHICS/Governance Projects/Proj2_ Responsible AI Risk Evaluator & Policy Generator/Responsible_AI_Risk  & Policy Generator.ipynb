{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJODrxd4ddFauQVj27W4U5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1LfuRSRn36xi"},"outputs":[],"source":["# Governance Project 2: Responsible AI Risk Evaluator & Policy Generator\n","\n","# Step 1: Define AI system (Healthcare AI using LIME)\n","ai_system = {\n","    \"name\": \"Healthcare Risk Classification AI\",\n","    \"use_case\": \"Disease risk prediction\",\n","    \"bias_mitigated\": True,\n","    \"explainability_tool\": \"LIME\",\n","    \"global_explainability\": False,\n","    \"human_oversight\": False,\n","    \"privacy_measures\": True,\n","    \"transparency_doc\": True,\n","    \"developer\": \"SoulMindFusion Ethical AI Lab\"\n","}\n","\n","# Step 2: Evaluate Responsible AI Pillars\n","risk_flags = {}\n","score = 0\n","\n","# Bias\n","risk_flags[\"Bias\"] = \"Low (Mitigated)\" if ai_system[\"bias_mitigated\"] else \"High (Unmitigated)\"\n","score += 1 if ai_system[\"bias_mitigated\"] else 0\n","\n","# Explainability\n","if ai_system[\"explainability_tool\"] == \"SHAP\" and ai_system[\"global_explainability\"]:\n","    risk_flags[\"Explainability\"] = \"Strong (Global & Local)\"\n","    score += 1\n","elif ai_system[\"explainability_tool\"] == \"LIME\":\n","    risk_flags[\"Explainability\"] = \"Moderate (Local Only)\"\n","    score += 0.5\n","else:\n","    risk_flags[\"Explainability\"] = \"Weak or Missing\"\n","\n","# Transparency\n","risk_flags[\"Transparency\"] = \"Good\" if ai_system[\"transparency_doc\"] else \"Missing\"\n","score += 1 if ai_system[\"transparency_doc\"] else 0\n","\n","# Human Oversight\n","risk_flags[\"Human Oversight\"] = \"Missing\" if not ai_system[\"human_oversight\"] else \"Present\"\n","score += 1 if ai_system[\"human_oversight\"] else 0\n","\n","# Privacy\n","risk_flags[\"Privacy\"] = \"Compliant\" if ai_system[\"privacy_measures\"] else \"At Risk\"\n","score += 1 if ai_system[\"privacy_measures\"] else 0\n","\n","# Step 3: Assign Responsible AI Score\n","max_score = 4.5\n","rating = \"\"\n","if score == max_score:\n","    rating = \"A (Fully Responsible AI System)\"\n","elif score >= 3.5:\n","    rating = \"B (Ethically Acceptable, Minor Gaps)\"\n","else:\n","    rating = \"C (Needs Governance Improvements)\"\n","\n","# Step 4: Print Summary\n","print(\"Responsible AI Risk Flag Summary for:\", ai_system[\"name\"])\n","for pillar, status in risk_flags.items():\n","    print(f\"- {pillar}: {status}\")\n","\n","print(f\"\\nResponsible AI Score: {score}/{max_score}\")\n","print(\"Rating:\", rating)\n","\n","# Step 5: Export Recommendation JSON\n","import json\n","summary = {\n","    \"Project\": ai_system[\"name\"],\n","    \"Use Case\": ai_system[\"use_case\"],\n","    \"Risk Flags\": risk_flags,\n","    \"Responsible AI Score\": f\"{score}/{max_score}\",\n","    \"Rating\": rating,\n","    \"Developer\": ai_system[\"developer\"]\n","}\n","\n","with open(\"responsible_ai_policy.json\", \"w\") as f:\n","    json.dump(summary, f, indent=4)\n","\n","print(\"\\nPolicy summary saved as 'responsible_ai_policy.json'\")\n"]}]}