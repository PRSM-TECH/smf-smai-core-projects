# Gender Bias in Job Hiring AI

## ğŸ“‹ Project Overview
This project focuses on detecting and mitigating gender bias in AI-driven hiring decisions. The model initially favored male applicants, and fairness was improved using bias mitigation techniques.

- **Model**: Logistic Regression
- **Fairness Metric**: Demographic Parity Difference (DPD)
- **Dataset**: 1000 synthetic resumes
- **Bias Mitigation**: Exponentiated Gradient (Fairlearn)

## ğŸ“Š Key Results
- **Before Bias Fix**: DPD = 0.8 | Accuracy = 100%
- **After Bias Fix**: DPD = 0.6 | Accuracy = 90%

## ğŸ›  Techniques Used
- Model: `LogisticRegression()`
- Bias Fix: `ExponentiatedGradient()`
- Bias Metric: `demographic_parity_difference()`
- Accuracy: `accuracy_score()`

## âš ï¸ Challenges
- SettingWithCopyWarning during encoding
- Over-compensation towards females initially
- Ethical review for borderline cases

## âœ… Fixes
- Used `.loc` for encoding
- Tuned fairness constraints
- Manual review for SMF alignment

## ğŸ§  Key Learnings
- Fairness is more than numbers â€“ ethical intent matters.
- SMF principles guided deeper fairness evaluation.
